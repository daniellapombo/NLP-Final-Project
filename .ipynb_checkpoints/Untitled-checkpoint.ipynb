{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP Algorithms for Reddit Text Data\n",
      "Objective: identify age demographics of the author and commentor\n",
      "Goal: achieve a evalution metric of greater than 65%\n",
      "\n",
      "\n",
      "Code:\n",
      "\n",
      "1. Import data ---> import_txtF() \n",
      "2. Preprocess (clean) data  \n",
      "    i. Removal of unwanted characters, splits data ---> rdttxt_processing() \n",
      "    i. Removal of unwanted characters, splits data ---> rdttxt_processing() \n",
      "3. Format data for NLP and ML algorithms \n",
      "i. Word2Vec, and Tf-idf\n",
      "4. Train various MLA and NLP's:\n",
      "i. KKN \n",
      "ii. Bayesian  \n",
      "iii. CNN \n",
      "iv. NN \n",
      "5. Analyze efficency of algorithms on development set\n",
      "i. hyperparameter selection \n",
      "ii. model selection\n",
      "6. Process and analysize model on test data\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Authors:\n",
    "Danie\n",
    "Paulina\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#CNN\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "#Preprocessing\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from scipy.stats import norm, expon, skewnorm, stats\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "#Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.utils import lemmatize\n",
    "from gensim.utils import is_corpus\n",
    "\n",
    "#Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "#Model Evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import  auc\n",
    "\n",
    "print(\"NLP Algorithms for Reddit Text Data\" +\n",
    "\"\\nObjective: identify age demographics of the author and commentor\" +\n",
    "\"\\nGoal: achieve a evalution metric of greater than 65%\\n\\n\" +\n",
    "\"\\nCode:\\n\" +\n",
    "    \"\\n1. Import data ---> import_txtF()\" +\n",
    "   \" \\n2. Preprocess (clean) data \" +\n",
    "      \" \\n    i. Removal of unwanted characters, splits data ---> rdttxt_processing()\" +\n",
    "       \" \\n    i. Removal of unwanted characters, splits data ---> rdttxt_processing()\" +\n",
    "      \n",
    "  \" \\n3. Format data for NLP and ML algorithms\" +\n",
    "      \" \\ni. Word2Vec, and Tf-idf\" +\n",
    "    \"\\n4. Train various MLA and NLP's:\"\n",
    "        \"\\ni. KKN\"\n",
    "       \" \\nii. Bayesian\"\n",
    "      \"  \\niii. CNN\"\n",
    "       \" \\niv. NN\"\n",
    "   \" \\n5. Analyze efficency of algorithms on development set\"\n",
    "        \"\\ni. hyperparameter selection\"\n",
    "       \" \\nii. model selection\"\n",
    "    \"\\n6. Process and analysize model on test data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import data sets and assign corresponding labels\n",
    "'teen_processed.csv' - reddit teen text\n",
    "'genx_processed.csv' - reddit GenX text\n",
    "'''\n",
    "    \n",
    "def import_txtF():\n",
    "    teen = pd.read_csv('./data/teen_processed.csv', header = [0])\n",
    "    tn_label = np.zeros((teen.shape[0], 1), dtype=int)\n",
    "    gX = pd.read_csv('./data/genx_processed.csv', header = [0])\n",
    "    gX_label = np.ones((gX.shape[0], 1), dtype=int)\n",
    "\n",
    "    return teen, tn_label, gX, gX_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Splits dataset up and labels data accordingly\n",
    "Removes unwanted characters\n",
    "Resets index and shuffle\n",
    "Returns split data\n",
    "'''\n",
    "\n",
    "def rdttxt_processing():\n",
    "    tn, tn_l, genX, genX_l = import_txtF()\n",
    "    \n",
    "    #Create dataset (combining all data into one data set of text and labels)\n",
    "    dat = pd.concat([tn, genX])\n",
    "    dat = dat.drop(dat.columns[[0]], axis = 1)\n",
    "    dat['labels'] = np.concatenate([tn_l, genX_l])\n",
    "    \n",
    "    #Clean data set of unwanted characters\n",
    "    dat.replace(to_replace = r'[0-9\"\\*><\\',]', value = ' ', regex = True, inplace = True)       \n",
    "    \n",
    "    #Shuffle data set\n",
    "    dat = dat.sample(frac=1).reset_index(drop=True)\n",
    "    doc = dat.iloc[:,0]\n",
    "    \n",
    "    train_txt, test_txt, train_label, test_label = train_test_split(doc, dat['labels'], test_size= .2, train_size = .8, shuffle = True)\n",
    "    return train_txt, test_txt, train_label, test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " trn, test, trn_label, tst_label = rdttxt_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Takes corpus and returns a vector by computing idf values\n",
    "Inverse Document Frequency (IDF) is a weight indicating how commonly a word is used. \n",
    "The more frequent its usage across documents, the lower its score. \n",
    "The lower the score, the less important/unique the word becomes\n",
    "IDF screenshots can be found under the Documents folder\n",
    "\n",
    "In practice, your IDF should be based on a large corpora of text.\n",
    "\n",
    "compute tf-idf scores: weighs a term's trquency (TF) and its inverse document frequency IDF\n",
    "#gets the word counts for the documents in a sparse matrix form\n",
    "\n",
    " all the words in this document have a tf-idf score and everything else show up as zeroes.\n",
    " \n",
    " Tfidfvectorizer you compute the word counts, idf and tf-idf values all at once. It’s really simple.\n",
    "Stop words are initialized with settings\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def count_vec(doc): \n",
    "    tfidf_vectorizer=TfidfVectorizer(use_idf=True,stop_words='english',max_df = 1, min_df = 1)\n",
    "    return tfidf_vectorizer, tfidf_vectorizer.fit_transform(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_____',\n",
       " '_how_',\n",
       " '_is_',\n",
       " '_lyfe_',\n",
       " '_removed_',\n",
       " 'a_____',\n",
       " 'aa',\n",
       " 'aaaaaa',\n",
       " 'aaaaaaaaaaa',\n",
       " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'aaaaaaand',\n",
       " 'aaal',\n",
       " 'aadventures',\n",
       " 'aang',\n",
       " 'abbaccio',\n",
       " 'abc',\n",
       " 'abdul',\n",
       " 'abhor',\n",
       " 'abide',\n",
       " 'abiding',\n",
       " 'abort',\n",
       " 'abs',\n",
       " 'absent',\n",
       " 'absenteeism',\n",
       " 'absolately',\n",
       " 'absoluteunit',\n",
       " 'absolutley',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'abstaining',\n",
       " 'absurdity',\n",
       " 'abues',\n",
       " 'abusers',\n",
       " 'abusive',\n",
       " 'accelerated',\n",
       " 'accents',\n",
       " 'accessibility',\n",
       " 'accessory',\n",
       " 'accidents',\n",
       " 'accomplishing',\n",
       " 'accomplishments',\n",
       " 'accord',\n",
       " 'accounts',\n",
       " 'accoustics',\n",
       " 'accredited',\n",
       " 'accumulated',\n",
       " 'accustomed',\n",
       " 'accutrooper',\n",
       " 'ache',\n",
       " 'achieve',\n",
       " 'achievement',\n",
       " 'achtung',\n",
       " 'acidic',\n",
       " 'acids',\n",
       " 'acknowledge',\n",
       " 'acorns',\n",
       " 'acouple',\n",
       " 'acquire',\n",
       " 'actaully',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'acute',\n",
       " 'adams',\n",
       " 'adaptations',\n",
       " 'adapted',\n",
       " 'adapting',\n",
       " 'adblock',\n",
       " 'addendum',\n",
       " 'adderal',\n",
       " 'adderall',\n",
       " 'addled',\n",
       " 'ade',\n",
       " 'adenine',\n",
       " 'adhd',\n",
       " 'adjunct',\n",
       " 'adjust',\n",
       " 'adjustable',\n",
       " 'adjusted',\n",
       " 'adlibs',\n",
       " 'admin',\n",
       " 'administer',\n",
       " 'adopter',\n",
       " 'adrenalin',\n",
       " 'adrenaline',\n",
       " 'adrian',\n",
       " 'adrift',\n",
       " 'adtech',\n",
       " 'adulterating',\n",
       " 'adultery',\n",
       " 'adulting',\n",
       " 'advancement',\n",
       " 'advancements',\n",
       " 'advantages',\n",
       " 'advertise',\n",
       " 'advertisements',\n",
       " 'advertises',\n",
       " 'advertising',\n",
       " 'adverts',\n",
       " 'advised',\n",
       " 'advocate',\n",
       " 'advocates',\n",
       " 'aerosols',\n",
       " 'aesthetic',\n",
       " 'affair',\n",
       " 'affection',\n",
       " 'affiliated',\n",
       " 'affirmation',\n",
       " 'afk',\n",
       " 'afloat',\n",
       " 'afoot',\n",
       " 'africa',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'ag',\n",
       " 'agador',\n",
       " 'agaric',\n",
       " 'agatha',\n",
       " 'agencies',\n",
       " 'agey',\n",
       " 'agrees',\n",
       " 'agriculture',\n",
       " 'ags',\n",
       " 'aha',\n",
       " 'ahahhahaha',\n",
       " 'ahhhhh',\n",
       " 'ahhhhhh',\n",
       " 'ahhhhhhhhh',\n",
       " 'ahhhhhhhhhhhhhhhh',\n",
       " 'aighthth',\n",
       " 'ailments',\n",
       " 'airborne',\n",
       " 'airdrop',\n",
       " 'airing',\n",
       " 'airjab',\n",
       " 'airpods',\n",
       " 'airports',\n",
       " 'aisle',\n",
       " 'ajay',\n",
       " 'ajr',\n",
       " 'ak',\n",
       " 'akabur',\n",
       " 'akai',\n",
       " 'akin',\n",
       " 'akron',\n",
       " 'ala',\n",
       " 'alabamaians',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alaska',\n",
       " 'alberto',\n",
       " 'albino',\n",
       " 'alcholol',\n",
       " 'alcoholics',\n",
       " 'alert',\n",
       " 'alerts',\n",
       " 'algae',\n",
       " 'algebra',\n",
       " 'alice',\n",
       " 'aliemnts',\n",
       " 'alienist',\n",
       " 'alignment',\n",
       " 'allegations',\n",
       " 'alleges',\n",
       " 'allergenic',\n",
       " 'allergy',\n",
       " 'alles',\n",
       " 'allocated',\n",
       " 'allocation',\n",
       " 'allowances',\n",
       " 'allulose',\n",
       " 'alooot',\n",
       " 'alpaca',\n",
       " 'alphabet',\n",
       " 'alphabetising',\n",
       " 'alrighty',\n",
       " 'altar',\n",
       " 'alter',\n",
       " 'alteration',\n",
       " 'altered',\n",
       " 'alternating',\n",
       " 'alts',\n",
       " 'alwys',\n",
       " 'alyssa',\n",
       " 'amani',\n",
       " 'ambience',\n",
       " 'ambiguous',\n",
       " 'amcas',\n",
       " 'ame',\n",
       " 'amen',\n",
       " 'amendment',\n",
       " 'amerikkkan',\n",
       " 'amino',\n",
       " 'amirite',\n",
       " 'ammo',\n",
       " 'amore',\n",
       " 'amphetamine',\n",
       " 'amphetamines',\n",
       " 'amplify',\n",
       " 'amsterdam',\n",
       " 'amt',\n",
       " 'amyls',\n",
       " 'anabolic',\n",
       " 'anaconda',\n",
       " 'anaesthesia',\n",
       " 'analog',\n",
       " 'analogues',\n",
       " 'analyze',\n",
       " 'anatomy',\n",
       " 'ancestors',\n",
       " 'anderson',\n",
       " 'andre',\n",
       " 'andrews',\n",
       " 'androgenous',\n",
       " 'andrsde',\n",
       " 'anesthesia',\n",
       " 'anf',\n",
       " 'angora',\n",
       " 'angrier',\n",
       " 'angsty',\n",
       " 'anguish',\n",
       " 'ann',\n",
       " 'annexed',\n",
       " 'annihilation',\n",
       " 'annnndddd',\n",
       " 'announcements',\n",
       " 'announcing',\n",
       " 'annoyance',\n",
       " 'annually',\n",
       " 'anomaly',\n",
       " 'ant',\n",
       " 'anteater',\n",
       " 'anthems',\n",
       " 'anthrax',\n",
       " 'antibiotic',\n",
       " 'anticipated',\n",
       " 'anticipation',\n",
       " 'antics',\n",
       " 'antidepressants',\n",
       " 'antimalarial',\n",
       " 'antiope',\n",
       " 'antioxidants',\n",
       " 'antiperspirant',\n",
       " 'anubis',\n",
       " 'anvil',\n",
       " 'anxious',\n",
       " 'anyday',\n",
       " 'anymo',\n",
       " 'anywho',\n",
       " 'aomeone',\n",
       " 'apathetic',\n",
       " 'apb',\n",
       " 'apc',\n",
       " 'apdb',\n",
       " 'apocalypse',\n",
       " 'apologizes',\n",
       " 'apologizing',\n",
       " 'apowogizing',\n",
       " 'apparent',\n",
       " 'appealing',\n",
       " 'appearing',\n",
       " 'appetising',\n",
       " 'applaud',\n",
       " 'applause',\n",
       " 'apples',\n",
       " 'applicable',\n",
       " 'appoint',\n",
       " 'appointment',\n",
       " 'approached',\n",
       " 'approx',\n",
       " 'appt',\n",
       " 'aqueous',\n",
       " 'ar',\n",
       " 'arabiano',\n",
       " 'arachidonic',\n",
       " 'arachnophobia',\n",
       " 'arbitrary',\n",
       " 'arch',\n",
       " 'archaic',\n",
       " 'archero',\n",
       " 'archetype',\n",
       " 'archetypes',\n",
       " 'archie',\n",
       " 'arching',\n",
       " 'architecture',\n",
       " 'arcs',\n",
       " 'arent',\n",
       " 'argentina',\n",
       " 'argentine',\n",
       " 'arguement',\n",
       " 'arise',\n",
       " 'armed',\n",
       " 'armies',\n",
       " 'armours',\n",
       " 'armoyr',\n",
       " 'aroind',\n",
       " 'aromatic',\n",
       " 'arose',\n",
       " 'arousal',\n",
       " 'aroused',\n",
       " 'arrangement',\n",
       " 'arrangements',\n",
       " 'arrest',\n",
       " 'arresting',\n",
       " 'arrived',\n",
       " 'arrivederci',\n",
       " 'arrives',\n",
       " 'arron',\n",
       " 'arrows',\n",
       " 'artemis',\n",
       " 'articulate',\n",
       " 'artifact',\n",
       " 'artifacts',\n",
       " 'artistic',\n",
       " 'aru',\n",
       " 'as_sdt',\n",
       " 'as_vis',\n",
       " 'aselmble',\n",
       " 'asexual',\n",
       " 'ash',\n",
       " 'asia',\n",
       " 'asians',\n",
       " 'askmen',\n",
       " 'askouija',\n",
       " 'askwomen',\n",
       " 'asl',\n",
       " 'asparagus',\n",
       " 'aspirin',\n",
       " 'assassinate',\n",
       " 'assassins',\n",
       " 'assembled',\n",
       " 'assert',\n",
       " 'assessments',\n",
       " 'asshat',\n",
       " 'asshats',\n",
       " 'assignment',\n",
       " 'assistant',\n",
       " 'assisted',\n",
       " 'associate',\n",
       " 'association',\n",
       " 'assortment',\n",
       " 'assumption',\n",
       " 'assumptions',\n",
       " 'assured',\n",
       " 'asterix',\n",
       " 'asthmatic',\n",
       " 'astley',\n",
       " 'astra',\n",
       " 'astrological',\n",
       " 'astroturfer',\n",
       " 'astroworld',\n",
       " 'atake',\n",
       " 'atbge',\n",
       " 'athletes',\n",
       " 'atlanta',\n",
       " 'atlus',\n",
       " 'atm',\n",
       " 'atmosphere',\n",
       " 'atoms',\n",
       " 'atp',\n",
       " 'attain',\n",
       " 'attempted',\n",
       " 'attend',\n",
       " 'attidude',\n",
       " 'attitudes',\n",
       " 'attorney',\n",
       " 'attosecond',\n",
       " 'attract',\n",
       " 'attrition',\n",
       " 'atttt',\n",
       " 'atty',\n",
       " 'auction',\n",
       " 'audition',\n",
       " 'audrey',\n",
       " 'aunt',\n",
       " 'aussies',\n",
       " 'austintown',\n",
       " 'australian',\n",
       " 'authentic',\n",
       " 'authorization',\n",
       " 'autocorrected',\n",
       " 'autographs',\n",
       " 'automatic',\n",
       " 'automation',\n",
       " 'autonomic',\n",
       " 'autumn',\n",
       " 'auxotrophy',\n",
       " 'avail',\n",
       " 'avant',\n",
       " 'ave',\n",
       " 'avenge',\n",
       " 'avenged',\n",
       " 'avenue',\n",
       " 'avgn',\n",
       " 'avid',\n",
       " 'avocado',\n",
       " 'avulsion',\n",
       " 'awaiting',\n",
       " 'awaits',\n",
       " 'awakening',\n",
       " 'awd',\n",
       " 'awe',\n",
       " 'awesomeness',\n",
       " 'awfully',\n",
       " 'awkwardly',\n",
       " 'awkwardness',\n",
       " 'awnser',\n",
       " 'awwws',\n",
       " 'awwww',\n",
       " 'ay',\n",
       " 'ayahuasca',\n",
       " 'ayayayaya',\n",
       " 'ayayayayay',\n",
       " 'ayayayayaya',\n",
       " 'ayyyy',\n",
       " 'ayyyyy',\n",
       " 'ayyyyyyy',\n",
       " 'babyfaces',\n",
       " 'babysitters',\n",
       " 'baccy',\n",
       " 'bachelors',\n",
       " 'backbreaker',\n",
       " 'backdodged',\n",
       " 'backdoor',\n",
       " 'backdropped',\n",
       " 'backhanded',\n",
       " 'backing',\n",
       " 'backpack',\n",
       " 'backround',\n",
       " 'backs',\n",
       " 'backstabbed',\n",
       " 'backstreet',\n",
       " 'backyardigans',\n",
       " 'bacon',\n",
       " 'bacteria',\n",
       " 'bada',\n",
       " 'badger',\n",
       " 'bae',\n",
       " 'baffled',\n",
       " 'bagged',\n",
       " 'bagging',\n",
       " 'baguette',\n",
       " 'bahah',\n",
       " 'bahamas',\n",
       " 'baiter',\n",
       " 'balancing',\n",
       " 'balenci',\n",
       " 'ballad',\n",
       " 'ballbusting',\n",
       " 'ballistas',\n",
       " 'ballot',\n",
       " 'ballou',\n",
       " 'bandage',\n",
       " 'bandages',\n",
       " 'bandaid',\n",
       " 'bandits',\n",
       " 'bane',\n",
       " 'bangbangbangbangbangbangbangbangbang',\n",
       " 'bangkok',\n",
       " 'bangladesh',\n",
       " 'bangs',\n",
       " 'banjo',\n",
       " 'banjos',\n",
       " 'bankrupt',\n",
       " 'bankruptcies',\n",
       " 'bankruptcy',\n",
       " 'banks',\n",
       " 'banshee',\n",
       " 'banter',\n",
       " 'barack',\n",
       " 'barba',\n",
       " 'barbarian',\n",
       " 'barefoot',\n",
       " 'barenaked',\n",
       " 'barfy',\n",
       " 'barker',\n",
       " 'barley',\n",
       " 'barnes',\n",
       " 'barre',\n",
       " 'barrel',\n",
       " 'barricade',\n",
       " 'bartender',\n",
       " 'baseline',\n",
       " 'basf',\n",
       " 'bashed',\n",
       " 'bashing',\n",
       " 'basicallyyy',\n",
       " 'basilica',\n",
       " 'basset',\n",
       " 'bassett',\n",
       " 'bastard',\n",
       " 'bastet',\n",
       " 'bastid',\n",
       " 'batch',\n",
       " 'batgirl',\n",
       " 'bath',\n",
       " 'bathing',\n",
       " 'baths',\n",
       " 'bathtub',\n",
       " 'batista',\n",
       " 'batshit',\n",
       " 'battlecamo',\n",
       " 'batwoman',\n",
       " 'bavaria',\n",
       " 'bayo',\n",
       " 'bball',\n",
       " 'bbc',\n",
       " 'bbn',\n",
       " 'bbses',\n",
       " 'bcus',\n",
       " 'bday',\n",
       " 'bdays',\n",
       " 'bdon',\n",
       " 'bdsm',\n",
       " 'beaches',\n",
       " 'beastiality',\n",
       " 'beastie',\n",
       " 'beaulo',\n",
       " 'beautifully',\n",
       " 'bebop',\n",
       " 'bedding',\n",
       " 'beds',\n",
       " 'bedspread',\n",
       " 'beef',\n",
       " 'beefyboicoguar',\n",
       " 'beerus',\n",
       " 'bees',\n",
       " 'beethoven',\n",
       " 'beetle',\n",
       " 'beetlejuicing',\n",
       " 'beets',\n",
       " 'beggars',\n",
       " 'begging',\n",
       " 'begins',\n",
       " 'begone',\n",
       " 'begs',\n",
       " 'behaving',\n",
       " 'behest',\n",
       " 'bein',\n",
       " 'belair',\n",
       " 'belarus',\n",
       " 'belated',\n",
       " 'belgium',\n",
       " 'believes',\n",
       " 'belinda',\n",
       " 'bellyand',\n",
       " 'belts',\n",
       " 'bench',\n",
       " 'bend',\n",
       " 'bender',\n",
       " 'bending',\n",
       " 'bene',\n",
       " 'beneath',\n",
       " 'benign',\n",
       " 'benioff',\n",
       " 'bent',\n",
       " 'benzo',\n",
       " 'benzodiazepine',\n",
       " 'benzodiazepines',\n",
       " 'benzofuran',\n",
       " 'benzos',\n",
       " 'benzylpiperazine',\n",
       " 'berated',\n",
       " 'berg',\n",
       " 'bern',\n",
       " 'bernardino',\n",
       " 'bertus',\n",
       " 'besidesz',\n",
       " 'bested',\n",
       " 'bestest',\n",
       " 'bestfriend',\n",
       " 'besties',\n",
       " 'beth',\n",
       " 'betray',\n",
       " 'betrayed',\n",
       " 'betterment',\n",
       " 'betting',\n",
       " 'bevvy',\n",
       " 'beyonce',\n",
       " 'bfby',\n",
       " 'bhang',\n",
       " 'bhappy',\n",
       " 'bianca',\n",
       " 'biases',\n",
       " 'bicentennial',\n",
       " 'bid',\n",
       " 'bidets',\n",
       " 'biked',\n",
       " 'bikini',\n",
       " 'bilie',\n",
       " 'bilingual',\n",
       " 'billboards',\n",
       " 'billions',\n",
       " 'bills',\n",
       " 'bin',\n",
       " 'binary',\n",
       " 'binders',\n",
       " 'binge',\n",
       " 'binged',\n",
       " 'bingo',\n",
       " 'binmen',\n",
       " 'biochemist',\n",
       " 'biodegradable',\n",
       " 'biolage',\n",
       " 'biosynthesis',\n",
       " 'biotech',\n",
       " 'biracial',\n",
       " 'birdcage',\n",
       " 'birdfeeder',\n",
       " 'birdseed',\n",
       " 'biscuit',\n",
       " 'biscuits',\n",
       " 'bisexuals',\n",
       " 'bismuth',\n",
       " 'bison',\n",
       " 'bistro',\n",
       " 'bitemoji',\n",
       " 'bittaker',\n",
       " 'bitten',\n",
       " 'bjj',\n",
       " 'blackadder',\n",
       " 'blackberries',\n",
       " 'blacked',\n",
       " 'blackheads',\n",
       " 'blackness',\n",
       " 'blake',\n",
       " 'blamed',\n",
       " 'blanc',\n",
       " 'blank',\n",
       " 'blanked',\n",
       " 'blasphemous',\n",
       " 'blastoise',\n",
       " 'blasts',\n",
       " 'blazeit',\n",
       " 'blazers',\n",
       " 'bleeds',\n",
       " 'bleh',\n",
       " 'blemish',\n",
       " 'blend',\n",
       " 'blender',\n",
       " 'blessings',\n",
       " 'bleu',\n",
       " 'blinded',\n",
       " 'blinding',\n",
       " 'bliss',\n",
       " 'blissfully',\n",
       " 'blitz',\n",
       " 'blizzards',\n",
       " 'blob',\n",
       " 'blocker',\n",
       " 'blocks',\n",
       " 'blogs',\n",
       " 'blonde',\n",
       " 'bloodfest',\n",
       " 'bloodstream',\n",
       " 'bloodweb',\n",
       " 'bloom',\n",
       " 'bloomberg',\n",
       " 'blossom',\n",
       " 'blotter',\n",
       " 'blowfish',\n",
       " 'blowjob',\n",
       " 'blowtorches',\n",
       " 'blueberries',\n",
       " 'blues',\n",
       " 'blunders',\n",
       " 'blur',\n",
       " 'blurry',\n",
       " 'bluster',\n",
       " 'bm',\n",
       " 'bmi',\n",
       " 'bmw',\n",
       " 'bnl',\n",
       " 'boards',\n",
       " 'boating',\n",
       " 'boatman',\n",
       " 'boats',\n",
       " 'boba',\n",
       " 'bobbed',\n",
       " 'bobbie',\n",
       " 'bod',\n",
       " 'bodied',\n",
       " 'bodily',\n",
       " 'bodyguard',\n",
       " 'boeing',\n",
       " 'bogart',\n",
       " 'boggling',\n",
       " 'boils',\n",
       " 'boing',\n",
       " 'bol',\n",
       " 'bolognese',\n",
       " 'bolt',\n",
       " 'bom',\n",
       " 'bombay',\n",
       " 'bombing',\n",
       " 'bonding',\n",
       " 'boned',\n",
       " 'bonehurtingjuice',\n",
       " 'bonesaw',\n",
       " 'bong',\n",
       " 'bonjour',\n",
       " 'bonkers',\n",
       " 'bonquerors',\n",
       " 'bonsai',\n",
       " 'bonsoir',\n",
       " 'boob',\n",
       " 'boobssss',\n",
       " 'boogaloo',\n",
       " 'boogeyman',\n",
       " 'boogiepop',\n",
       " 'booing',\n",
       " 'bookcases',\n",
       " 'booker',\n",
       " 'booklets',\n",
       " 'booma',\n",
       " 'boomerhentai',\n",
       " 'boooooooooooooo',\n",
       " 'booster',\n",
       " 'booths',\n",
       " 'bootied',\n",
       " 'bootstrap',\n",
       " 'booze',\n",
       " 'bop',\n",
       " 'bopped',\n",
       " 'bordered',\n",
       " 'borderline',\n",
       " 'borla',\n",
       " 'boron',\n",
       " 'bos',\n",
       " 'bosg',\n",
       " 'botany',\n",
       " 'bothering',\n",
       " 'bothersome',\n",
       " 'bothh',\n",
       " 'bottoms',\n",
       " 'bouken',\n",
       " 'bounces',\n",
       " 'bound',\n",
       " 'boundless',\n",
       " 'bounties',\n",
       " 'bounty',\n",
       " 'bouts',\n",
       " 'boxes',\n",
       " 'boyfriends',\n",
       " 'bra',\n",
       " 'bracket',\n",
       " 'brad',\n",
       " 'braddoesbanter',\n",
       " 'brag',\n",
       " 'brah',\n",
       " 'braid',\n",
       " 'braincells',\n",
       " 'braindead',\n",
       " 'brained',\n",
       " 'brainer',\n",
       " 'brainlet',\n",
       " 'braking',\n",
       " 'branch',\n",
       " 'branches',\n",
       " 'branded',\n",
       " 'brandi',\n",
       " 'brands',\n",
       " 'brats',\n",
       " 'braver',\n",
       " 'bravest',\n",
       " 'brazilian',\n",
       " 'brb',\n",
       " 'breakables',\n",
       " 'breakdown',\n",
       " 'breakdowns',\n",
       " 'breaker',\n",
       " 'breakers',\n",
       " 'breakup',\n",
       " 'breathable',\n",
       " 'breeding',\n",
       " 'breezula',\n",
       " 'breezy',\n",
       " 'brewing',\n",
       " 'brexit',\n",
       " 'bridging',\n",
       " 'brienne',\n",
       " 'brightness',\n",
       " 'brim',\n",
       " 'brine',\n",
       " 'brisk',\n",
       " 'britain',\n",
       " 'brittany',\n",
       " 'broadcast',\n",
       " 'broaden',\n",
       " 'broader',\n",
       " 'broadly',\n",
       " 'broadway',\n",
       " 'broccoli',\n",
       " 'bronco',\n",
       " 'brooklyn',\n",
       " 'brotha',\n",
       " 'browne',\n",
       " 'brtual',\n",
       " 'bruno',\n",
       " 'brushed',\n",
       " 'brushing',\n",
       " 'brussels',\n",
       " 'brutality',\n",
       " 'brücke',\n",
       " 'bsa',\n",
       " 'bsr',\n",
       " 'bssm',\n",
       " 'bte',\n",
       " 'bubblegum',\n",
       " 'bubbly',\n",
       " 'buckaroo',\n",
       " 'buckle',\n",
       " 'buckshot',\n",
       " 'buddys',\n",
       " 'budgets',\n",
       " 'budgies',\n",
       " 'buds',\n",
       " 'buffon',\n",
       " 'buffoon',\n",
       " 'bugger',\n",
       " 'bugging',\n",
       " 'buggy',\n",
       " 'bulb',\n",
       " 'bulbs',\n",
       " 'bulgari',\n",
       " 'bulge',\n",
       " 'bulger',\n",
       " 'bulk',\n",
       " 'bullets',\n",
       " 'bullies',\n",
       " 'bullshitter',\n",
       " 'bulma',\n",
       " 'bumfuck',\n",
       " 'bumped',\n",
       " 'bumper',\n",
       " 'bun',\n",
       " 'bunches',\n",
       " 'bundled',\n",
       " 'burback',\n",
       " 'burbs',\n",
       " 'burgers',\n",
       " 'burial',\n",
       " 'buries',\n",
       " 'burlington',\n",
       " 'burnham',\n",
       " 'burnt',\n",
       " 'burrows',\n",
       " 'burton',\n",
       " 'burying',\n",
       " 'bushwhacked',\n",
       " 'busted',\n",
       " 'buster',\n",
       " 'butane',\n",
       " 'butcher',\n",
       " 'butterface',\n",
       " 'butters',\n",
       " 'butthole',\n",
       " 'buttload',\n",
       " 'buttocks',\n",
       " 'buttons',\n",
       " 'butylone',\n",
       " 'buyers',\n",
       " 'bworry',\n",
       " 'byproduct',\n",
       " 'byrne',\n",
       " 'bzp',\n",
       " 'büttõń',\n",
       " 'cab',\n",
       " 'cabal',\n",
       " 'cabinet',\n",
       " 'cabinets',\n",
       " 'cabot',\n",
       " 'cackles',\n",
       " 'cadaver',\n",
       " 'cadet',\n",
       " 'cafe',\n",
       " 'cafeteria',\n",
       " 'cagematch',\n",
       " 'cahoots',\n",
       " 'caillou',\n",
       " 'cakeday',\n",
       " 'cal',\n",
       " 'calabasas',\n",
       " 'calc',\n",
       " 'cali',\n",
       " 'caliber',\n",
       " 'calibrated',\n",
       " 'californication',\n",
       " 'caliphate',\n",
       " 'caller',\n",
       " 'calvin',\n",
       " 'cam',\n",
       " 'cambridge',\n",
       " 'camper',\n",
       " 'camps',\n",
       " 'campuses',\n",
       " 'canadians',\n",
       " 'canadien',\n",
       " 'canceling',\n",
       " 'cancellation',\n",
       " 'cancelling',\n",
       " 'cancels',\n",
       " 'candice',\n",
       " 'candidacy',\n",
       " 'cane',\n",
       " 'cannabinoids',\n",
       " 'canned',\n",
       " 'cannons',\n",
       " 'canola',\n",
       " 'canon',\n",
       " 'canteen',\n",
       " 'capability',\n",
       " 'capers',\n",
       " 'capital',\n",
       " 'capitalising',\n",
       " 'capitão',\n",
       " 'capsules',\n",
       " 'captain',\n",
       " 'captions',\n",
       " 'captivity',\n",
       " 'caramel',\n",
       " 'carbohydrates',\n",
       " 'carbohydratesstapledtotrees',\n",
       " 'carby',\n",
       " 'cardboard',\n",
       " 'cards',\n",
       " 'careers',\n",
       " 'caress',\n",
       " 'carfentanyl',\n",
       " 'carnivores',\n",
       " 'carol',\n",
       " 'carolina',\n",
       " 'caroline',\n",
       " 'carpets',\n",
       " 'carries',\n",
       " 'carrol',\n",
       " 'carrying',\n",
       " 'cartman',\n",
       " 'cartridges',\n",
       " 'carves',\n",
       " 'cashier',\n",
       " 'cashing',\n",
       " 'cashmere',\n",
       " 'casket',\n",
       " 'cassingle',\n",
       " 'casted',\n",
       " 'castle',\n",
       " 'catalyzed',\n",
       " 'catapult',\n",
       " 'catatonia',\n",
       " 'catback',\n",
       " 'catches',\n",
       " 'catching',\n",
       " 'catchphrase',\n",
       " 'catered',\n",
       " 'caterpillar',\n",
       " 'cathinone',\n",
       " 'cathinones',\n",
       " 'catson',\n",
       " 'catty',\n",
       " 'cauliflower',\n",
       " 'caution',\n",
       " 'cavalier',\n",
       " 'cave',\n",
       " 'cazadores',\n",
       " 'cb',\n",
       " 'cbd',\n",
       " 'cbt',\n",
       " 'cc',\n",
       " 'ce',\n",
       " 'ceiling',\n",
       " 'celebrate',\n",
       " 'celebrities',\n",
       " 'cellos',\n",
       " 'celtic',\n",
       " 'cemeteries',\n",
       " 'cenobite',\n",
       " 'centerline',\n",
       " 'central',\n",
       " 'centres',\n",
       " 'centurions',\n",
       " 'cereal',\n",
       " 'cereals',\n",
       " 'cero',\n",
       " 'certification',\n",
       " 'cesspool',\n",
       " 'chained',\n",
       " 'chainsaw',\n",
       " 'challenging',\n",
       " 'chamomile',\n",
       " 'champ',\n",
       " 'champions',\n",
       " 'championships',\n",
       " 'chancellors',\n",
       " 'chanel',\n",
       " 'chang',\n",
       " 'channels',\n",
       " 'chaotic',\n",
       " 'chap',\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_raw, tfidf_vector_transformed = count_vec(trn)\n",
    "# Raw word features\n",
    "# vector_raw.get_feature_names()\n",
    "def wrd2vc():\n",
    "    \n",
    "    return W2VTransformer(size=300, window = 3, min_count = 3, sg = 1, trim_rule = lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "\n",
    "def wrd2vc():\n",
    "    \n",
    "    return W2VTransformer(size=300, window = 3, min_count = 3, sg = 1, trim_rule = lemmatize)\n",
    "    \n",
    "#\n",
    "#class MeanEmbeddingVectorizer(object): #http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "def __init__(self, word2vec):\n",
    "   self.word2vec = word2vec\n",
    "   # if a text is empty we should return a vector of zeros\n",
    "   # with the same dimensionality as all the other vectors\n",
    "   self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "def fit(self, X, y):\n",
    "   return self\n",
    "\n",
    "def transform(self, X):\n",
    "   return np.array([np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "               or [np.zeros(self.dim)], axis=0) for words in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def models(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    vectoriz = [vec(), wrd2vc()]\n",
    "    \n",
    "    names = ['LogisticReg', 'KNN', 'Naive Bayes', 'Multi layer Preceptrion']\n",
    "    \n",
    "    classifiers = [LogisticRegression(max_iter=100), KNeighborsClassifier(), ComplementNB(), \n",
    "                   MLPClassifier(hidden_layer_sizes = (20,3), max_iter= 100)]\n",
    "    \n",
    "    parameters = [{'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(0, 4, 10)},\n",
    "                {'classifier__n_neighbors': [2, 5, 10, 50, 100]}, \n",
    "                {'classifier__alpha': [0, 0.5, 1, 5]}, \n",
    "                {'classifier__activation': ['logistic', 'relu'], \n",
    "                 'classifier__solver': ['adam', 'sgd'], \n",
    "                 'classifier__alpha': [0, 0.001, 0.5, 1]}]\n",
    "    \n",
    "    parameters = [{'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(0, 4, 3)}]\n",
    "    \n",
    "#    #Record results/findings (data)\n",
    "    out_file = open('Comp329ModelEvaluationReport.txt', 'w')\n",
    "    \n",
    "    out_file.writelines('Classifier Report' + '\\n')\n",
    "    \n",
    "    for name, model, param in zip(names, classifiers, parameters):\n",
    "        for preprocessor in vectoriz:\n",
    "            print(classifiers)\n",
    "            pipe = Pipeline(steps = [('preprocessor', preprocessor),('classifier', model)])\n",
    "            pipe.fit(X_train, y_train)   \n",
    "            \n",
    "            clf = GridSearchCV(pipe, param, cv=5, verbose=0, refit = True).fit(X_train, y_train)\n",
    "            print(clf.best_estimator_)\n",
    "            clf = clf.best_estimator_\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            out_file.writelines(name + ' Results on test data' +'using '  + str(preprocessor) + 'tokenizer' + '\\n')\n",
    "            #out_file.writelines(clf + '\\n')\n",
    "            out_file.writelines('Accuracy ' + str(np.mean(y_pred == y_test)) + '\\n')\n",
    "            out_file.writelines(classification_report(y_test, y_pred) + '\\n')\n",
    "            PRE, REC, _ = precision_recall_curve(y_test, y_pred, pos_label = 1) #Precision recall curve returns precision, recall and its threshold\n",
    "            AUC = auc(REC, PRE) #Compute area under precision recall curve\n",
    "            out_file.writelines('AUC results ' + str(AUC) + '\\n')\n",
    "            out_file.writelines('\\n')\n",
    "    \n",
    "    \n",
    "    out_file.close()\n",
    "    \n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform'), ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(20, 3), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=100,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)]\n",
      "Pipeline(memory=None,\n",
      "         steps=[('preprocessor',\n",
      "                 TfidfVectorizer(analyzer=<function vec.<locals>.stems at 0x14485f7b8>,\n",
      "                                 binary=False, decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words='english', strip_ac...\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=100.0, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=100,\n",
      "                                    multi_class='auto', n_jobs=None,\n",
      "                                    penalty='l2', random_state=None,\n",
      "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform'), ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(20, 3), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=100,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/gensim/utils.py:1684: UserWarning: The light flag is no longer supported by pattern.\n",
      "  warnings.warn(\"The light flag is no longer supported by pattern.\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'oh  and maybe a tumbleweed going past too?' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ff19eed44534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#do we thinkg '>', *' are important?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-ff19eed44534>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdttxt_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-0ada765a4453>\u001b[0m in \u001b[0;36mmodels\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preprocessor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'classifier'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/sklearn_api/w2vmodel.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/sklearn_api/w2vmodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'oh  and maybe a tumbleweed going past too?' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    trn, test, trn_label, tst_label = rdttxt_processing()\n",
    "   \n",
    "    models(trn, trn_label, test, tst_label)\n",
    "main()\n",
    "\n",
    "#do we thinkg '>', *' are important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
