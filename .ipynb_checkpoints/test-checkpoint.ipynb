{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import regexp_tokenize \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "#Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "#Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GenX = pd.read_csv('./data/original/GenX_sub_data.csv')\n",
    "Teen = pd.read_csv('./data/original/teenagers_sub_data.csv')\n",
    "\n",
    "def clean_data(data):\n",
    "    if (data['User'] != 'AutoModerator'):\n",
    "        data['Comment'] = data['Comment'].lower()\n",
    "        if('https://' not in data['Comment']):\n",
    "            return data\n",
    "    else:\n",
    "        data = data.drop('User')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdttxt_processing(GenX, Teen):\n",
    "    GenX = GenX.apply(clean_data, axis=1)\n",
    "    Teen = Teen.apply(clean_data, axis=1)\n",
    "\n",
    "    # drop deleted comments and unnecesssary columns\n",
    "    GenX = GenX.dropna()\n",
    "    Teen = Teen.dropna()\n",
    "\n",
    "    gX = GenX.drop(['Unnamed: 0', 'SubredditofOrgin', 'Submission', 'User'], axis=1)\n",
    "    teen = Teen.drop(['Unnamed: 0', 'SubredditofOrgin','Submission', 'User'], axis=1)\n",
    "    tn_l = np.zeros((teen.shape[0], 1), dtype=int)\n",
    "    teen['labels'] = tn_l\n",
    "    \n",
    "    gX_l = np.ones((gX.shape[0], 1), dtype=int)\n",
    "    gX['labels'] = gX_l\n",
    "  \n",
    "    #Create dataset (combining all data into one data set of text and labels)\n",
    "    dat = pd.concat([teen, gX])\n",
    "    dat = dat.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    #Clean data set of unwanted characters\n",
    "    dat['Comment'] = dat['Comment'].apply(lambda x: x.replace(r'[0-9\"\\*><\\',]', ''))  \n",
    "    dat['Comment'] = dat['Comment'].apply(lambda x: re.sub(' +', ' ',x))\n",
    "    \n",
    "    \n",
    "    #Shuffle data set\n",
    "    dat = dat.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    doc = dat.iloc[:,0]\n",
    "   \n",
    "    train_txt, test_txt, train_label, test_label = train_test_split(doc, dat['labels'], test_size= .2, train_size = .8,  random_state=42)\n",
    "    return train_txt, test_txt, train_label, test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.items())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.items())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "trn, test, trn_label, tst_label = rdttxt_processing(GenX, Teen)\n",
    "\n",
    "trn = trn.apply(regexp_tokenize, pattern=\"[\\w']+\")\n",
    "\n",
    "model = gensim.models.Word2Vec(trn, size=200, min_count=1, workers=4)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "\n",
    "etree_w2v = Pipeline([\n",
    "(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([\n",
    "(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "(\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"KNN\", KNeighborsClassifier())])\n",
    "nb = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "svm = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SGDClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-25d8fa299c3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mKNN_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSVM_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mSGD_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGDClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m print(\"Cross Validation Score Results \\n\\n\" + \n\u001b[1;32m      6\u001b[0m       \u001b[0;34mf\"Naive Bayes:{nb_s} %\\n\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SGDClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "nb_s = round(np.mean(cross_val_score(nb, test, tst_label))*100,2)\n",
    "KNN_s = round(np.mean(cross_val_score(KNN, test, tst_label))*100,2)\n",
    "SVM_s = round(np.mean(cross_val_score(svm, test, tst_label))*100,2)\n",
    "print(\"Cross Validation Score Results \\n\\n\" + \n",
    "      f\"Naive Bayes:{nb_s} %\\n\"+\n",
    "      f\"KNN:{KNN_s} %\\n\" +\n",
    "      f\"SVM:{SVM_s} %\\n\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
