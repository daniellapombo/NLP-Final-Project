{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "import * only allowed at module level (<ipython-input-8-00861e283b94>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-00861e283b94>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    import pandas as pd\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m import * only allowed at module level\n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import expon\n",
    "from scipy.stats import skewnorm\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "# Preprocessing\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import pattern3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# MLA and NLP Models\n",
    "#from keras.models import Sequential\n",
    "#from keras import layersfrom sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.utils import lemmatize\n",
    "from gensim.utils import is_corpus\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "# for LSTM Model\n",
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import *\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "import * only allowed at module level (<ipython-input-5-d881e3c579ba>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-d881e3c579ba>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    import numpy as np\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m import * only allowed at module level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"NLP Algorithms for Reddit Text Data\n",
    "Objective: identify age demographics of the author and commentor\n",
    "Goal: achieve a evalution metric of greater than 65%\n",
    "Code:\n",
    "    1. Import data\n",
    "    2. Preprocess (clean) data\n",
    "        i. Of unwanted characters, and information\n",
    "    3. Format data for NLP and ML algorithms\n",
    "        i. Word2Vec, and Tf-idf\n",
    "    4. Train various MLA and NLP's:\n",
    "        i. KKN\n",
    "        ii. Bayesian\n",
    "        iii. CNN\n",
    "        iv. NN\n",
    "    5. Analyze efficency of algorithms on development set\n",
    "        i. hyperparameter selection\n",
    "        ii. model selection\n",
    "    6. Process and analysize model on test data\n",
    "\"\"\"\n",
    "\n",
    "GenX = pd.read_csv('./data/original/GenX_sub_data.csv')\n",
    "Teen = pd.read_csv('./data/original/teenagers_sub_data.csv')\n",
    "\n",
    "# Removes bots, comments with links, and makes all comments lowercase\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    if (data['User'] != 'AutoModerator'):\n",
    "        data['Comment'] = data['Comment'].lower()\n",
    "        if('https://' not in data['Comment']):\n",
    "            return data\n",
    "    else:\n",
    "        data = data.drop('User')\n",
    "\n",
    "\n",
    "GenX = GenX.apply(clean_data, axis=1)\n",
    "Teen = Teen.apply(clean_data, axis=1)\n",
    "\n",
    "# drop deleted comments and unnecesssary columns\n",
    "GenX = GenX.dropna()\n",
    "Teen = Teen.dropna()\n",
    "\n",
    "GenX = GenX.drop(['Unnamed: 0', 'SubredditofOrgin',\n",
    "                  'Submission', 'User'], axis=1)\n",
    "Teen = Teen.drop(['Unnamed: 0', 'SubredditofOrgin',\n",
    "                  'Submission', 'User'], axis=1)\n",
    "\n",
    "GenX.to_csv('./data/genx_processed.csv')\n",
    "Teen.to_csv('./data/teen_processed.csv')\n",
    "\n",
    "\n",
    "def import_txtF():\n",
    "    # Import data sets and assign corresponding labels\n",
    "        # 'teen_processed.csv' - reddit teen text\n",
    "        # 'genx_processed.csv' - reddit GenX text\n",
    "\n",
    "    teen = pd.read_csv('./data/teen_processed.csv', header=[0])\n",
    "    tn_label = np.zeros((teen.shape[0], 1), dtype=int)\n",
    "    gX = pd.read_csv('./data/genx_processed.csv', header=[0])\n",
    "    gX_label = np.ones((gX.shape[0], 1), dtype=int)\n",
    "\n",
    "    return teen, tn_label, gX, gX_label\n",
    "\n",
    "\n",
    "def rdttxt_processing():\n",
    "    tn, tn_l, genX, genX_l = import_txtF()\n",
    "\n",
    "    # Create dataset (combining all data into one data set of text and labels)\n",
    "    dat = pd.concat([tn, genX])\n",
    "    dat = dat.drop(dat.columns[[0]], axis=1)\n",
    "    dat['labels'] = np.concatenate([tn_l, genX_l])\n",
    "\n",
    "    # Clean data set of unwanted characters\n",
    "    dat.replace(to_replace=r'[0-9\"\\*><\\',]',\n",
    "                value=' ', regex=True, inplace=True)\n",
    "\n",
    "    # Shuffle data set\n",
    "    dat = dat.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    doc = dat.iloc[:, 0]\n",
    "    doc = vec(doc)  # Vectorize text data\n",
    "\n",
    "    train_txt, test_txt, train_label, test_label = train_test_split(\n",
    "        doc, dat['labels'], test_size=.2, train_size=.8, shuffle=True)\n",
    "\n",
    "    return train_txt, test_txt, train_label, test_label\n",
    "\n",
    "\n",
    "def vec():\n",
    "    # Code inspired by https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn/36191362\n",
    "    analyzer = TfidfVectorizer().build_analyzer()\n",
    "\n",
    "    # Implements lemmalization funcationality to Vectorizer (tokenization & bag-of-words) method\n",
    "    def stems(doc):\n",
    "        ps = PorterStemmer()\n",
    "        return (ps.stem(w) for w in analyzer(doc))\n",
    "\n",
    "    # Intialize vectorizer\n",
    "    return TfidfVectorizer(stop_words='english', max_df=1, min_df=1, analyzer=stems)\n",
    "\n",
    "\n",
    "def wrd2vc():\n",
    "    return W2VTransformer(size=300, window=3, min_count=3, sg=1, trim_rule=lemmatize)\n",
    "\n",
    "\n",
    "def make_LSTM():\n",
    "    # gridsearch integration inspired by https://medium.com/@masonrchildress/how-to-gridsearch-over-a-keras-neural-network-with-a-pipeline-51fbfb62185e\n",
    "    max_features = 20000\n",
    "    maxlen = 100\n",
    "    batch_size = 64\n",
    "    x = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    print('Train...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=15,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "    return model\n",
    "\n",
    "\n",
    "def CNN():\n",
    "    embedding_dim = 100\n",
    "\n",
    "    # create architecture\n",
    "    cnn = Sequential()\n",
    "    cnn.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "    cnn.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "    cnn.add(layers.GlobalMaxPooling1D())\n",
    "    cnn.add(layers.Dense(10, activation='relu'))\n",
    "    cnn.add(layers.Dense(1, activation='sigmoid'))\n",
    "    cnn.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    history = cnn.fit(train_txt, train_label,\n",
    "                      epochs=10,\n",
    "                      verbose=False,\n",
    "                      validation_data=(test_txt, test_label),  batch_size=10)\n",
    "    loss, accuracy = model.evaluate(train_txt, train_label, verbose=False)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "    loss, accuracy = model.evaluate(test_txt, test_label, verbose=False)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "    plot_history(history)\n",
    "\n",
    "\n",
    "def models(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    vectoriz = [vec(), wrd2vc()]\n",
    "\n",
    "    names = ['LogisticReg', 'KNN', 'Naive Bayes',\n",
    "             'Multi layer Preceptrion', 'LSTM', 'CNN']\n",
    "\n",
    "    estimators = []\n",
    "    estimators.append(('pf', PolynomialFeatures(interaction_only=True,\n",
    "                                                include_bias=False)))\n",
    "    estimators.append(('ss', StandardScaler()))\n",
    "    estimators.append(('nn', KerasClassifier(\n",
    "        build_fn=make_LSTM, epochs=10, batch_size=5, verbose=0)))\n",
    "    estimators.append(('nn', KerasClassifier(\n",
    "        build_fn=CNN, epochs=10, batch_size=5, verbose=0)))\n",
    "    nn_pipe = Pipeline(estimators)\n",
    "\n",
    "    classifiers = [LogisticRegression(max_iter=100), KNeighborsClassifier(), ComplementNB(),\n",
    "                   MLPClassifier(hidden_layer_sizes=(20, 3), max_iter=100), KerasClassifier(build_fn='make_LSTM'),KerasClassifier(build_fn='CNN')]\n",
    "\n",
    "    parameters = [{'classifier__penalty': ['l1', 'l2'],\n",
    "                   'classifier__C': np.logspace(0, 4, 10)},\n",
    "                  {'classifier__n_neighbors': [2, 5, 10, 50, 100]},\n",
    "                  {'classifier__alpha': [0, 0.5, 1, 5]},\n",
    "                  {'classifier__activation': ['logistic', 'relu'],\n",
    "                   'classifier__solver': ['adam', 'sgd'],\n",
    "                   'classifier__alpha': [0, 0.001, 0.5, 1]},\n",
    "                  {'nn__epochs': [2, 3, 5, 10], 'nn__batch_size':[1, 3, 5]}]\n",
    "\n",
    "    parameters = [{'classifier__penalty': ['l2'],\n",
    "                   'classifier__C': np.logspace(0, 4, 3)}]\n",
    "\n",
    "#    #Record results/findings (data)\n",
    "    out_file = open('Comp329ModelEvaluationReport.txt', 'w')\n",
    "\n",
    "    out_file.writelines('Classifier Report' + '\\n')\n",
    "\n",
    "    for name, model, param in zip(names, classifiers, parameters):\n",
    "        for preprocessor in vectoriz:\n",
    "            print(classifiers)\n",
    "            pipe = Pipeline(\n",
    "                steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "            pipe.fit(X_train, y_train)\n",
    "\n",
    "            clf = GridSearchCV(pipe, param, cv=5, verbose=0,\n",
    "                               refit=True).fit(X_train, y_train)\n",
    "            print(clf.best_estimator_)\n",
    "            clf = clf.best_estimator_\n",
    "            y_pred = clf.predict(X_test)\n",
    "\n",
    "            out_file.writelines(name + ' Results on test data' +\n",
    "                                'using ' + str(preprocessor) + 'tokenizer' + '\\n')\n",
    "            #out_file.writelines(clf + '\\n')\n",
    "            out_file.writelines(\n",
    "                'Accuracy ' + str(np.mean(y_pred == y_test)) + '\\n')\n",
    "            out_file.writelines(classification_report(y_test, y_pred) + '\\n')\n",
    "            # Precision recall curve returns precision, recall and its threshold\n",
    "            PRE, REC, _ = precision_recall_curve(y_test, y_pred, pos_label=1)\n",
    "            AUC = auc(REC, PRE)  # Compute area under precision recall curve\n",
    "            out_file.writelines('AUC results ' + str(AUC) + '\\n')\n",
    "            out_file.writelines('\\n')\n",
    "\n",
    "    out_file.close()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    trn, test, trn_label, tst_label = rdttxt_processing()\n",
    "    models(trn, trn_label, test, tst_label)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
