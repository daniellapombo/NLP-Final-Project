{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP FINAL PROJECT\n",
    "#ANGIE GEORGARAS, PAULINA ADAMNSKI, DANIELLA POMBO, LINETTE MALIAKAL, ALEX ROSE, JACK BROOKS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "GenX = pd.read_csv('./data/original/GenX_sub_data.csv')\n",
    "Teen = pd.read_csv('./data/original/teenagers_sub_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes bots, comments with links, and makes all comments lowercase\n",
    "def clean_data(data):\n",
    "    if (data['User'] != 'AutoModerator'):\n",
    "        data['Comment'] = data['Comment'].lower()\n",
    "        if('https://' not in data['Comment']):\n",
    "            return data\n",
    "    else:\n",
    "            data = data.drop('User')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "GenX = GenX.apply(clean_data,axis=1)\n",
    "Teen = Teen.apply(clean_data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop deleted comments and unnecesssary columns\n",
    "GenX = GenX.dropna()\n",
    "Teen = Teen.dropna()\n",
    "\n",
    "GenX = GenX.drop(['Unnamed: 0','SubredditofOrgin', 'Submission', 'User'], axis=1)\n",
    "Teen = Teen.drop(['Unnamed: 0','SubredditofOrgin', 'Submission', 'User'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "GenX.to_csv('./data/genx_processed.csv')\n",
    "Teen.to_csv('./data/teen_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_txtF():\n",
    "    #Import data sets and assign corresponding labels\n",
    "        #'teen_processed.csv' - reddit teen text\n",
    "        #'genx_processed.csv' - reddit GenX text\n",
    "    \n",
    "    teen = pd.read_csv('./data/teen_processed.csv', header = [0])\n",
    "    tn_label = np.zeros((teen.shape[0], 1), dtype=int)\n",
    "    gX = pd.read_csv('./data/genx_processed.csv', header = [0])\n",
    "    gX_label = np.ones((gX.shape[0], 1), dtype=int)\n",
    "\n",
    "    return teen, tn_label, gX, gX_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdttxt_processing():\n",
    "    tn, tn_l, genX, genX_l = import_txtF()\n",
    "    \n",
    "    #Create dataset (combining all data into one data set of text and labels)\n",
    "    dat = pd.concat([tn, genX])\n",
    "    dat = dat.drop(dat.columns[[0]], axis = 1)\n",
    "    dat['labels'] = np.concatenate([tn_l, genX_l])\n",
    "    \n",
    "    #Clean data set of unwanted characters\n",
    "    dat.replace(to_replace = r'[0-9\"\\*><\\',]', value = ' ', regex = True, inplace = True)       \n",
    "    \n",
    "    #Shuffle data set\n",
    "    dat = dat.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    doc = dat.iloc[:,0]\n",
    "    doc = vec(doc) #Vectorize text data\n",
    "    \n",
    "    train_txt, test_txt, train_label, test_label = train_test_split(doc, dat['labels'], test_size= .2, train_size = .8, shuffle = True)\n",
    "    \n",
    "    return train_txt, test_txt, train_label, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(trn, tst= None):\n",
    "    # Code inspired by https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn/36191362\n",
    "    \n",
    "    analyzer = TfidfVectorizer().build_analyzer()\n",
    "    \n",
    "    #Implements lemmalization funcationality to Vectorizer (tokenization & bag-of-words) method\n",
    "    def stems(doc):\n",
    "        ps = PorterStemmer() \n",
    "        return (ps.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "    #remove stop words (ANGIE)\n",
    "    \n",
    "    #Intialize vectorizer\n",
    "    vectorize = TfidfVectorizer(stop_words = 'english', max_df = 1, min_df = 1, analyzer = stems)\n",
    "    \n",
    "    vec_trn = vectorize.fit_transform(trn) #Fits and Transforms training data\n",
    "    vocab = vectorize.get_feature_names() #identifies the text (words) associated w/ each index value\n",
    "\n",
    "    #return vec_trn, vec_tst\n",
    "    return vec_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "def NB():\n",
    "    nbs = ComplementNB()\n",
    "    nbs.fit(train_txt, train_label)\n",
    "    \n",
    "    print(\"naive bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Classifier\n",
    "def LR():\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_txt, train_label)\n",
    "    \n",
    "    print(\"logistic regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Classifier (ANGIE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation (GRIDSEARCH = ANGIE)\n",
    "def model_evaluation():\n",
    "\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training/Tuning\n",
    "def model_training():\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    rdttxt_processing()\n",
    "    #Wild out B\n",
    "   \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
